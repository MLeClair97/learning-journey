{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f7d88f",
   "metadata": {},
   "source": [
    "Connections to local SQL Express"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2f0774",
   "metadata": {},
   "source": [
    "Intially this session was slated for using pyodbc but the intial execution showed a warning: \"UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy. \"  Adjusted to move forward with SQLAlchemy instead.\n",
    "\n",
    "- Old (pyodbc direct): Caused warnings\n",
    "    - \"DRIVER={ODBC Driver 17 for SQL Server};SERVER=server;DATABASE=db;Trusted_Connection=yes;\"\n",
    "\n",
    "- New (SQLAlchemy): No warnings, best practice\n",
    "    - \"mssql+pyodbc://@server/database?driver=ODBC+Driver+17+for+SQL+Server&trusted_connection=yes\"\n",
    "\n",
    "Updated code -  The first section is to test the connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a08019d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to SQL Server: localhost\\SQLEXPRESS\n",
      "SQL Server Version:\n",
      "                                  sql_server_version\n",
      "0  Microsoft SQL Server 2022 (RTM-GDR) (KB5058712...\n",
      "SQL Server connection closed\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "\n",
    "class SQLServerConnection:\n",
    "    \"\"\"SQL Server database connection using SQLAlchemy (recommended)\"\"\"\n",
    "    \n",
    "    def __init__(self, server, database, username=None, password=None, trusted_connection=True):\n",
    "        self.server = server\n",
    "        self.database = database\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.trusted_connection = trusted_connection\n",
    "        self.engine = None\n",
    "    \n",
    "    def get_connection_string(self):\n",
    "        \"\"\"Build SQLAlchemy connection string\"\"\"\n",
    "        if self.trusted_connection:\n",
    "            # Windows Authentication\n",
    "            connection_string = f\"mssql+pyodbc://@{self.server}/{self.database}?driver=ODBC+Driver+17+for+SQL+Server&trusted_connection=yes\"\n",
    "        else:\n",
    "            # SQL Server Authentication\n",
    "            encoded_password = urllib.parse.quote_plus(self.password)\n",
    "            connection_string = f\"mssql+pyodbc://{self.username}:{encoded_password}@{self.server}/{self.database}?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "        \n",
    "        return connection_string\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Create SQLAlchemy engine\"\"\"\n",
    "        try:\n",
    "            connection_string = self.get_connection_string()\n",
    "            self.engine = create_engine(connection_string)\n",
    "            \n",
    "            # Test the connection\n",
    "            with self.engine.connect() as conn:\n",
    "                conn.execute(text(\"SELECT 1\"))\n",
    "            \n",
    "            print(f\"Connected to SQL Server: {self.server}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"SQL Server connection error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def execute_query(self, query, params=None):\n",
    "        \"\"\"Execute query and return DataFrame - no more pandas pyodbc warnings!\"\"\"\n",
    "        if not self.engine:\n",
    "            print(\"No database connection\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            if params:\n",
    "                df = pd.read_sql_query(text(query), self.engine, params=params)\n",
    "            else:\n",
    "                df = pd.read_sql_query(text(query), self.engine)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Query execution error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close database connection\"\"\"\n",
    "        if self.engine:\n",
    "            self.engine.dispose()\n",
    "            print(\"SQL Server connection closed\")\n",
    "\n",
    "# Example connection (adjust for the environment)\n",
    "sql_server = SQLServerConnection(\n",
    "    server='localhost\\\\SQLEXPRESS',  # Or other server name\n",
    "    database='master',  # Replace with your database name\n",
    "    trusted_connection=True  # Uses Windows Authentication\n",
    ")\n",
    "\n",
    "\n",
    "# Or with SQL Server Authentication:\n",
    "# sql_server = SQLServerConnection(\n",
    "#     server='your-server.database.windows.net',\n",
    "#     database='BusinessDB',\n",
    "#     username='your-username',\n",
    "#     password='your-password',\n",
    "#     trusted_connection=False\n",
    "# )\n",
    "\n",
    "if sql_server.connect():\n",
    "    # Test with a simple query first\n",
    "    test_query = \"SELECT @@VERSION as sql_server_version\"\n",
    "    result = sql_server.execute_query(test_query)\n",
    "    if result is not None:\n",
    "        print(\"SQL Server Version:\")\n",
    "        print(result)\n",
    "    \n",
    "    sql_server.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15a5ed9",
   "metadata": {},
   "source": [
    "Now pulling data from SQL Express:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2dd9700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to BusinessDB successfully!\n",
      "Top Customers:\n",
      "   CustomerID                 CustomerName DeliveryPostalCode  TotalInvoices  \\\n",
      "0         401   Wingtip Toys (Head Office)              90625          22165   \n",
      "1           1  Tailspin Toys (Head Office)              90410          22316   \n",
      "2         995                Kumar Naicker              90515            112   \n",
      "3         953                     Linh Dao              90147            130   \n",
      "4         840           Rodrigo Figueiredo              90331            105   \n",
      "5         813                 Shyam Poddar              90695            113   \n",
      "6         835             Dominic Davignon              90192            116   \n",
      "7         900                   Lilli Sokk              90357            125   \n",
      "8         915                   Ana Florea              90140            109   \n",
      "9         911                  Liidia Lepp              90137            112   \n",
      "\n",
      "   TotalRevenue  \n",
      "0      97053.58  \n",
      "1      56435.84  \n",
      "2      13212.12  \n",
      "3       6728.65  \n",
      "4       5975.40  \n",
      "5       5767.83  \n",
      "6       5732.98  \n",
      "7       5689.31  \n",
      "8       5511.15  \n",
      "9       5422.25  \n",
      "\n",
      "Analysis complete! Found 10 top customers.\n"
     ]
    }
   ],
   "source": [
    "def setup_business_database():\n",
    "    \"\"\"Connect to the database and run queries using SQLAlchemy\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Create SQLAlchemy engine for SQL Server with Windows Authentication\n",
    "        server = 'localhost\\\\SQLEXPRESS'\n",
    "        database = 'WideWorldImporters'\n",
    "        \n",
    "        # Create connection string for Windows Authentication\n",
    "        connection_string = f\"mssql+pyodbc://@{server}/{database}?driver=ODBC+Driver+17+for+SQL+Server&trusted_connection=yes\"\n",
    "        engine = create_engine(connection_string)\n",
    "        \n",
    "        # Test connection and execute query\n",
    "        with engine.connect() as connection:\n",
    "            print(\"Connected to BusinessDB successfully!\")\n",
    "            \n",
    "            # SQL query to get top customers\n",
    "            query = \"\"\"\n",
    "            SELECT TOP 10\n",
    "                c.CustomerID,\n",
    "                c.CustomerName,\n",
    "                c.DeliveryPostalCode,\n",
    "                COUNT(t.InvoiceID) as TotalInvoices,\n",
    "                SUM(t.TransactionAmount) as TotalRevenue\n",
    "            FROM Sales.Customers c\n",
    "            LEFT JOIN Sales.CustomerTransactions t ON c.CustomerID = t.CustomerID\n",
    "            GROUP BY c.CustomerID, c.CustomerName, c.DeliveryPostalCode\n",
    "            ORDER BY SUM(t.TransactionAmount) DESC\n",
    "            \"\"\"\n",
    "            \n",
    "            # Execute query using pandas (eliminates the warning!)\n",
    "            results = pd.read_sql(query, engine)\n",
    "            \n",
    "            if not results.empty:\n",
    "                print(\"Top Customers:\")\n",
    "                print(results)\n",
    "                return results\n",
    "            else:\n",
    "                print(\"No results found.\")\n",
    "                return None\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Database connection or query failed: {e}\")\n",
    "        return None\n",
    "    \n",
    "# Call the business database function\n",
    "def run_business_analysis():\n",
    "    \"\"\"Run the business database analysis\"\"\"\n",
    "    results = setup_business_database()\n",
    "    if results is not None:\n",
    "        print(f\"\\nAnalysis complete! Found {len(results)} top customers.\")\n",
    "    else:\n",
    "        print(\"Analysis failed or returned no results.\")\n",
    "\n",
    "# Run the function to get results\n",
    "# results = setup_business_database()\n",
    "# Or use the wrapper function to get results with a message\n",
    "run_business_analysis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d5578a",
   "metadata": {},
   "source": [
    "Advanced Analytics with SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31637535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_server_advanced_analytics():\n",
    "    \"\"\"Perform advanced analytics on SQL Server data\"\"\"\n",
    "    \n",
    "    sql_server = SQLServerConnection(\n",
    "        server='localhost\\\\SQLEXPRESS',\n",
    "        database='WideWorldImporters',  \n",
    "        trusted_connection=True\n",
    "    )\n",
    "    \n",
    "    if sql_server.connect():\n",
    "        # Complex analytics query using SQL Server features\n",
    "        analytics_query = \"\"\"\n",
    "        WITH CustomerMetrics AS (\n",
    "            SELECT \n",
    "                    c.CustomerID,\n",
    "                    c.CustomerName,\n",
    "                    OrderAmount,\n",
    "                    COUNT(o.OrderID) as TotalOrders,\n",
    "                    SUM(sq1.OrderAmount) as TotalRevenue,\n",
    "                    AVG(sq1.OrderAmount) as AvgOrderValue,\n",
    "                    MIN(o.OrderDate) as FirstOrderDate,\n",
    "                    MAX(o.OrderDate) as LastOrderDate,\n",
    "                    DATEDIFF(day, MIN(o.OrderDate), MAX(o.OrderDate)) as CustomerLifespanDays\n",
    "            FROM Sales.Customers c\n",
    "            LEFT JOIN Sales.Orders o ON c.CustomerID = o.CustomerID\n",
    "            LEFT JOIN ( Select OrderID, (PickedQuantity * UnitPrice) as OrderAmount FROM Sales.OrderLines) sq1 on sq1.OrderID = o.OrderID\n",
    "            GROUP BY c.CustomerID, c.CustomerName, OrderAmount\n",
    "        ),\n",
    "        CustomerSegments AS (\n",
    "            SELECT \n",
    "                *,\n",
    "                CASE \n",
    "                    WHEN TotalRevenue > 2000 THEN 'Premium'\n",
    "                    WHEN TotalRevenue > 1000 THEN 'Standard'\n",
    "                    ELSE 'Basic'\n",
    "                END as CustomerSegment,\n",
    "                CASE \n",
    "                    WHEN CustomerLifespanDays > 0 \n",
    "                    THEN TotalRevenue / CustomerLifespanDays * 365 \n",
    "                    ELSE TotalRevenue \n",
    "                END as EstimatedAnnualValue\n",
    "            FROM CustomerMetrics\n",
    "        )\n",
    "        SELECT \n",
    "            CustomerSegment,\n",
    "            COUNT(*) as CustomerCount,\n",
    "            AVG(TotalRevenue) as AvgRevenue,\n",
    "            AVG(EstimatedAnnualValue) as AvgAnnualValue,\n",
    "            SUM(TotalRevenue) as SegmentRevenue\n",
    "        FROM CustomerSegments\n",
    "        GROUP BY CustomerSegment\n",
    "        ORDER BY AvgRevenue DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        results = sql_server.execute_query(analytics_query)\n",
    "        if results is not None:\n",
    "            print(\"Customer Segment Analysis:\")\n",
    "            print(results)\n",
    "            \n",
    "            # Enhanced analysis with pandas\n",
    "            results['revenue_percentage'] = (results['SegmentRevenue'] / results['SegmentRevenue'].sum()) * 100\n",
    "            print(\"\\nRevenue Distribution by Segment:\")\n",
    "            print(results[['CustomerSegment', 'revenue_percentage']])\n",
    "        \n",
    "        sql_server.close()\n",
    "        return results\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Run advanced analytics\n",
    "analytics_results = sql_server_advanced_analytics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3544cc0f",
   "metadata": {},
   "source": [
    "Handling Large Datasets with Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29594dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_large_sql_server_dataset(query, chunk_size=10000):\n",
    "    \"\"\"Process large SQL Server datasets efficiently\"\"\"\n",
    "    \n",
    "    sql_server = SQLServerConnection(\n",
    "        server='localhost\\\\SQLEXPRESS',\n",
    "        database='WideWorldImporters',\n",
    "        trusted_connection=True\n",
    "    )\n",
    "    \n",
    "    if sql_server.connect():\n",
    "        try:\n",
    "            # Use chunking for large datasets\n",
    "            chunk_iterator = pd.read_sql_query(\n",
    "                text(query),\n",
    "                sql_server.engine,\n",
    "                chunksize=chunk_size\n",
    "            )\n",
    "            \n",
    "            processed_data = []\n",
    "            \n",
    "            for chunk_num, chunk_df in enumerate(chunk_iterator):\n",
    "                print(f\"Processing chunk {chunk_num + 1}, size: {len(chunk_df)}\")\n",
    "                \n",
    "                # Process each chunk\n",
    "                chunk_summary = {\n",
    "                    'chunk_number': chunk_num + 1,\n",
    "                    'row_count': len(chunk_df),\n",
    "                    'total_amount': chunk_df.get('OrderAmount', pd.Series([0])).sum(),\n",
    "                    'avg_amount': chunk_df.get('OrderAmount', pd.Series([0])).mean()\n",
    "                }\n",
    "                \n",
    "                processed_data.append(chunk_summary)\n",
    "            \n",
    "            sql_server.close()\n",
    "            \n",
    "            # Combine results\n",
    "            summary_df = pd.DataFrame(processed_data)\n",
    "            return summary_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Chunk processing error: {e}\")\n",
    "            sql_server.close()\n",
    "            return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Example usage for large datasets\n",
    "large_query = \"\"\"\n",
    "SELECT \n",
    "    CustomerID,\n",
    "    OrderDate,\n",
    "    SalespersonPersonID\n",
    "FROM Sales.Orders \n",
    "WHERE OrderDate >= '2020-01-01'\n",
    "ORDER BY OrderDate\n",
    "\"\"\"\n",
    "\n",
    "# Process in chunks\n",
    "chunk_summary = process_large_sql_server_dataset(large_query, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1dea5e",
   "metadata": {},
   "source": [
    "When I asked Claude.ai how to handle aggregates on chunked data, I got the following notes.  Adding here for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfde7599",
   "metadata": {},
   "source": [
    "# Aggregating Chunk Summaries\n",
    "\n",
    "When processing large datasets with chunking, you often end up with **chunk-level summaries** rather than the raw data. To get meaningful insights about your entire dataset, you need to properly aggregate these chunk summaries.\n",
    "\n",
    "## Understanding Chunk Summaries\n",
    "\n",
    "After chunking, your `summary_df` contains statistics for each individual chunk:\n",
    "\n",
    "```python\n",
    "# Example chunk summary output\n",
    "chunk_number  row_count  total_amount  avg_amount\n",
    "1             10000      50000.00      5.00\n",
    "2             10000      75000.00      7.50\n",
    "3             8500       42500.00      5.00\n",
    "```\n",
    "\n",
    "**Important**: These are chunk-level metrics, not overall dataset metrics.\n",
    "\n",
    "## Basic Aggregation Functions\n",
    "\n",
    "### Simple Aggregates Function\n",
    "\n",
    "```python\n",
    "def get_overall_aggregates(summary_df):\n",
    "    \"\"\"Calculate overall aggregates from chunk summaries\"\"\"\n",
    "    \n",
    "    # Total rows across all chunks\n",
    "    total_rows = summary_df['row_count'].sum()\n",
    "    \n",
    "    # Total amount across all chunks\n",
    "    total_amount = summary_df['total_amount'].sum()\n",
    "    \n",
    "    # Overall average (weighted by chunk sizes)\n",
    "    overall_avg = total_amount / total_rows\n",
    "    \n",
    "    return {\n",
    "        'total_rows': total_rows,\n",
    "        'total_amount': total_amount,\n",
    "        'overall_average': overall_avg,\n",
    "        'chunks_processed': len(summary_df)\n",
    "    }\n",
    "```\n",
    "\n",
    "### Usage Example\n",
    "\n",
    "```python\n",
    "# Process your large dataset\n",
    "chunk_summary = process_large_sql_server_dataset(large_query, 1000)\n",
    "\n",
    "if chunk_summary is not None:\n",
    "    # Get overall aggregates\n",
    "    aggregates = get_overall_aggregates(chunk_summary)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Total rows: {aggregates['total_rows']:,}\")\n",
    "    print(f\"Total amount: ${aggregates['total_amount']:,.2f}\")\n",
    "    print(f\"Overall average: ${aggregates['overall_average']:.2f}\")\n",
    "    print(f\"Chunks processed: {aggregates['chunks_processed']}\")\n",
    "```\n",
    "\n",
    "## Enhanced Aggregation with Detailed Statistics\n",
    "\n",
    "### Enhanced Processing Function\n",
    "\n",
    "```python\n",
    "def process_large_sql_server_dataset_enhanced(query, chunk_size=10000):\n",
    "    \"\"\"Enhanced version with more detailed chunk tracking\"\"\"\n",
    "    \n",
    "    sql_server = SQLServerConnection(\n",
    "        server='localhost\\\\SQLEXPRESS',\n",
    "        database='BusinessTestDB',\n",
    "        trusted_connection=True\n",
    "    )\n",
    "    \n",
    "    if sql_server.connect():\n",
    "        try:\n",
    "            chunk_iterator = pd.read_sql_query(\n",
    "                text(query),\n",
    "                sql_server.engine,\n",
    "                chunksize=chunk_size\n",
    "            )\n",
    "            \n",
    "            processed_data = []\n",
    "            running_totals = {'rows': 0, 'amount': 0}\n",
    "            \n",
    "            for chunk_num, chunk_df in enumerate(chunk_iterator):\n",
    "                print(f\"Processing chunk {chunk_num + 1}, size: {len(chunk_df)}\")\n",
    "                \n",
    "                # Detailed chunk analysis\n",
    "                order_amounts = chunk_df.get('OrderAmount', pd.Series([0]))\n",
    "                \n",
    "                chunk_summary = {\n",
    "                    'chunk_number': chunk_num + 1,\n",
    "                    'row_count': len(chunk_df),\n",
    "                    'total_amount': order_amounts.sum(),\n",
    "                    'avg_amount': order_amounts.mean(),\n",
    "                    'min_amount': order_amounts.min(),\n",
    "                    'max_amount': order_amounts.max(),\n",
    "                    'std_amount': order_amounts.std()\n",
    "                }\n",
    "                \n",
    "                # Update running totals\n",
    "                running_totals['rows'] += len(chunk_df)\n",
    "                running_totals['amount'] += order_amounts.sum()\n",
    "                \n",
    "                processed_data.append(chunk_summary)\n",
    "            \n",
    "            sql_server.close()\n",
    "            \n",
    "            # Create summary DataFrame\n",
    "            summary_df = pd.DataFrame(processed_data)\n",
    "            \n",
    "            # Calculate overall aggregates\n",
    "            overall_stats = {\n",
    "                'total_rows': running_totals['rows'],\n",
    "                'total_amount': running_totals['amount'],\n",
    "                'overall_average': running_totals['amount'] / running_totals['rows'] if running_totals['rows'] > 0 else 0,\n",
    "                'chunks_processed': len(summary_df),\n",
    "                'global_min': summary_df['min_amount'].min(),\n",
    "                'global_max': summary_df['max_amount'].max()\n",
    "            }\n",
    "            \n",
    "            return summary_df, overall_stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Chunk processing error: {e}\")\n",
    "            sql_server.close()\n",
    "            return None, None\n",
    "    \n",
    "    return None, None\n",
    "```\n",
    "\n",
    "### Enhanced Usage Example\n",
    "\n",
    "```python\n",
    "# Process with enhanced statistics\n",
    "chunk_summary, overall_stats = process_large_sql_server_dataset_enhanced(large_query, 1000)\n",
    "\n",
    "if chunk_summary is not None:\n",
    "    print(\"=== OVERALL STATISTICS ===\")\n",
    "    print(f\"Total rows processed: {overall_stats['total_rows']:,}\")\n",
    "    print(f\"Total amount: ${overall_stats['total_amount']:,.2f}\")\n",
    "    print(f\"Overall average: ${overall_stats['overall_average']:.2f}\")\n",
    "    print(f\"Global min: ${overall_stats['global_min']:.2f}\")\n",
    "    print(f\"Global max: ${overall_stats['global_max']:.2f}\")\n",
    "    print(f\"Chunks processed: {overall_stats['chunks_processed']}\")\n",
    "    \n",
    "    print(\"\\n=== CHUNK BREAKDOWN ===\")\n",
    "    display(chunk_summary)\n",
    "```\n",
    "\n",
    "## Aggregation Rules by Metric Type\n",
    "\n",
    "### Additive Metrics\n",
    "✅ **Simply sum across chunks**\n",
    "- Row counts: `summary_df['row_count'].sum()`\n",
    "- Total amounts: `summary_df['total_amount'].sum()`\n",
    "- Any cumulative values\n",
    "\n",
    "### Averages\n",
    "⚠️ **Must be weighted by chunk sizes**\n",
    "```python\n",
    "# ❌ Wrong - this gives average of averages\n",
    "wrong_avg = summary_df['avg_amount'].mean()\n",
    "\n",
    "# ✅ Correct - weighted average\n",
    "total_amount = summary_df['total_amount'].sum()\n",
    "total_rows = summary_df['row_count'].sum()\n",
    "correct_avg = total_amount / total_rows\n",
    "```\n",
    "\n",
    "### Min/Max Values\n",
    "✅ **Take min/max across all chunk values**\n",
    "```python\n",
    "global_min = summary_df['min_amount'].min()\n",
    "global_max = summary_df['max_amount'].max()\n",
    "```\n",
    "\n",
    "### Standard Deviation\n",
    "⚠️ **Complex - requires special handling**\n",
    "```python\n",
    "# For standard deviation, you typically need:\n",
    "# 1. Raw data access, or\n",
    "# 2. Additional statistics from each chunk (sum of squares, etc.)\n",
    "# Simple chunk std aggregation is not mathematically valid\n",
    "```\n",
    "\n",
    "## Stream Processing Alternative\n",
    "\n",
    "For simpler aggregation needs, consider calculating running totals during processing:\n",
    "\n",
    "```python\n",
    "def stream_process_dataset(query, chunk_size=10000):\n",
    "    \"\"\"Process data with running aggregates - no chunk storage\"\"\"\n",
    "    \n",
    "    # Initialize running aggregates\n",
    "    total_rows = 0\n",
    "    total_amount = 0.0\n",
    "    global_min = float('inf')\n",
    "    global_max = float('-inf')\n",
    "    \n",
    "    sql_server = SQLServerConnection(\n",
    "        server='localhost\\\\SQLEXPRESS',\n",
    "        database='BusinessTestDB',\n",
    "        trusted_connection=True\n",
    "    )\n",
    "    \n",
    "    if sql_server.connect():\n",
    "        try:\n",
    "            chunk_iterator = pd.read_sql_query(\n",
    "                text(query),\n",
    "                sql_server.engine,\n",
    "                chunksize=chunk_size\n",
    "            )\n",
    "            \n",
    "            for chunk_num, chunk_df in enumerate(chunk_iterator):\n",
    "                amounts = chunk_df.get('OrderAmount', pd.Series([0]))\n",
    "                \n",
    "                # Update running totals\n",
    "                total_rows += len(chunk_df)\n",
    "                total_amount += amounts.sum()\n",
    "                global_min = min(global_min, amounts.min())\n",
    "                global_max = max(global_max, amounts.max())\n",
    "                \n",
    "                # Progress update\n",
    "                if chunk_num % 10 == 0:  # Every 10 chunks\n",
    "                    print(f\"Processed {total_rows:,} rows so far...\")\n",
    "            \n",
    "            sql_server.close()\n",
    "            \n",
    "            return {\n",
    "                'total_rows': total_rows,\n",
    "                'total_amount': total_amount,\n",
    "                'overall_average': total_amount / total_rows if total_rows > 0 else 0,\n",
    "                'global_min': global_min if global_min != float('inf') else 0,\n",
    "                'global_max': global_max if global_max != float('-inf') else 0\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Stream processing error: {e}\")\n",
    "            sql_server.close()\n",
    "            return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Usage\n",
    "results = stream_process_dataset(large_query, 1000)\n",
    "if results:\n",
    "    print(f\"Final results: {results}\")\n",
    "```\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Chunk summaries are intermediate results** - always aggregate them for final statistics\n",
    "2. **Different metrics require different aggregation methods** - sums vs. averages vs. min/max\n",
    "3. **Weighted averages are crucial** - never average the averages directly\n",
    "4. **Consider stream processing** for simpler use cases where you don't need chunk-level details\n",
    "5. **Complex statistics** (like standard deviation) may require raw data access or specialized formulas\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Implement the appropriate aggregation method for your use case\n",
    "- Test with smaller datasets first to verify your aggregation logic\n",
    "- Consider adding data validation to ensure aggregation accuracy\n",
    "- Monitor memory usage if storing chunk summaries for very large datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
